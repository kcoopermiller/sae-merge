{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am_6oGvv6jkq"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YydISNwuaCvu"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login\n",
        "!pip install -qU huggingface_hub\n",
        "!sudo apt install python3-venv -y\n",
        "!pip install git+https://github.com/callummcdougall/eindex.git\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
        "\n",
        "!export LANG=ja_JP.UTF-8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOQj20Sc6nVY"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-LAnslv6kpT"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"aloobun/Reyna-Mini-1.8B-v0.2\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"aloobun/Reyna-Mini-1.8B-v0.2\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vAwTozGWEBp"
      },
      "source": [
        "## [Nejumi LLM Leaderboard Benchmark](https://wandb.ai/wandb-japan/llm-leaderboard/reports/Nejumi-LLM-Leaderboard-Evaluating-Japanese-Language-Proficiency--Vmlldzo2MzU3NzIy#features-of-the-nejumi-leaderboard-%F0%9F%90%80)\n",
        "\n",
        "Check Japanese profiency of base model\n",
        "\n",
        "You'll need to update `llm-leaderboard/configs/config.yaml` with the following information:\n",
        "```yaml\n",
        "wandb:\n",
        "  entity: \"your/WANDB/entity\"\n",
        "  project: \"your/WANDB/project\"\n",
        "  run_name: \"your/WANDB/run_name\"\n",
        "\n",
        "...\n",
        "\n",
        "model:\n",
        "  pretrained_model_name_or_path: 'name of your model'\n",
        "\n",
        "...\n",
        "\n",
        "tokenizer:\n",
        "  pretrained_model_name_or_path: 'name of your tokenizer'\n",
        "\n",
        "...\n",
        "\n",
        "metainfo:\n",
        "  basemodel_name: \"your modelname\"\n",
        "  model_type: \"\" # {open llm, commercial api}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/wandb/llm-leaderboard.git\n",
        "!cd llm-leaderboard && git submodule init && git submodule update --remote\n",
        "!python3 -m venv llm-leaderboard/llmjp\n",
        "!source llm-leaderboard/llmjp/bin/activate && pip install -q -r llm-leaderboard/requirements.txt\n",
        "!cp llm-leaderboard/configs/config_template.yaml llm-leaderboard/configs/config.yaml"
      ],
      "metadata": {
        "id": "Q9r9GsDE1qZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LEUBBKtaPp3"
      },
      "outputs": [],
      "source": [
        "!cd llm-leaderboard && source llmjp/bin/activate && python3 scripts/run_eval.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKDgKeGUT1Y"
      },
      "source": [
        "# Sparse Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q transformer_lens sae-lens wandb accelerate\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
        "\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner"
      ],
      "metadata": {
        "id": "1I6tHUGfyucj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://jbloomaus.github.io/SAELens/training_saes/\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name = \"qwen1.5-0.5b\",\n",
        "    hook_point = \"blocks.18.hook_resid_pre\",\n",
        "    hook_point_layer = 18,\n",
        "    d_in = 1024, # https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html\n",
        "    dataset_path = \"Skylion007/openwebtext\",\n",
        "    is_dataset_tokenized=False,\n",
        "\n",
        "    # SAE Parameters\n",
        "    expansion_factor = 64,\n",
        "    b_dec_init_method = \"geometric_median\",\n",
        "\n",
        "    # Training Parameters\n",
        "    lr = 0.0004,\n",
        "    l1_coefficient = 0.00008,\n",
        "    lr_scheduler_name=\"constant\", # constantwithwarmup not supported?\n",
        "    train_batch_size = 4096,\n",
        "    context_size = 128,\n",
        "    lr_warm_up_steps=5000,\n",
        "\n",
        "    # Activation Store Parameters\n",
        "    n_batches_in_buffer = 128,\n",
        "    total_training_tokens = 1_000_000 * 100,\n",
        "    store_batch_size = 32,\n",
        "\n",
        "    # Dead Neurons and Sparsity\n",
        "    use_ghost_grads=True,\n",
        "    feature_sampling_window = 1000,\n",
        "    dead_feature_window=5000,\n",
        "    dead_feature_threshold = 1e-6,\n",
        "\n",
        "    # WANDB\n",
        "    log_to_wandb = True,\n",
        "    wandb_project= \"mats_sae_training_qwen\",\n",
        "    wandb_entity = None,\n",
        "    wandb_log_frequency=100,\n",
        "\n",
        "    # Misc\n",
        "    device = \"cuda\",\n",
        "    seed = 42,\n",
        "    n_checkpoints = 10,\n",
        "    checkpoint_path = \"checkpoints\",\n",
        "    dtype = torch.float32,\n",
        "    )\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)\n"
      ],
      "metadata": {
        "id": "iRyD9P8vhMaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to HuggingFace\n"
      ],
      "metadata": {
        "id": "3O-U2hVwq9L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "uuid_str = \"pqs59n3e\"\n",
        "repo_id = \"kcoopermiller/qwen1.5-0.5b-saes\"\n",
        "local_folder = f\"checkpoints/{uuid_str}\"\n",
        "hf_folder = f\"{uuid_str}\"\n",
        "api.upload_folder(\n",
        "    folder_path=local_folder,\n",
        "    path_in_repo=hf_folder,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "id": "AIm4pugaownw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the SAEs"
      ],
      "metadata": {
        "id": "xrC3Evkfq3wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import plotly.express as px\n",
        "from transformer_lens import utils\n",
        "from datasets import load_dataset\n",
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download\n",
        "from functools import partial\n",
        "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
        "from sae_vis.data_fetching_fns import get_feature_data, FeatureData\n",
        "from sae_vis.data_config_classes import SaeVisConfig\n",
        "torch.set_grad_enabled(False)\n",
        "sys.path.append(\"..\")"
      ],
      "metadata": {
        "id": "oy3EcxQFgC4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REPO_ID = \"kcoopermiller/qwen1.5-0.5b-saes\"\n",
        "\n",
        "layer = 2 # 2 or 18\n",
        "checkpoint = \"crujwafo\" if layer == 2 else \"pqs59n3e\"\n",
        "FILENAME = f\"{checkpoint}/final_sae_group_qwen1.5-0.5b_blocks.{layer}.hook_resid_pre_65536.pt\"\n",
        "\n",
        "path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "\n",
        "model, sparse_autoencoders, activation_store = (\n",
        "    LMSparseAutoencoderSessionloader.load_session_from_pretrained(path=path)\n",
        ")\n",
        "sparse_autoencoders.eval()\n",
        "sparse_autoencoder = list(sparse_autoencoders)[0]"
      ],
      "metadata": {
        "id": "Nk7pZpt-iD93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L0 Test and Reconstruction Test"
      ],
      "metadata": {
        "id": "Q_wTDMvVrtIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
        "with torch.no_grad():\n",
        "    batch_tokens = activation_store.get_batch_tokens()\n",
        "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
        "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
        "        cache[sparse_autoencoder.cfg.hook_point]\n",
        "    )\n",
        "    del cache\n",
        "\n",
        "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
        "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
        "    print(\"average l0\", l0.mean().item())\n",
        "    px.histogram(l0.flatten().cpu().numpy()).show()\n",
        "\n",
        "# next we want to do a reconstruction test.\n",
        "def reconstr_hook(activation, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(activation, hook):\n",
        "    return torch.zeros_like(activation)\n",
        "\n",
        "\n",
        "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                utils.get_act_name(\"resid_pre\", 10),\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
        "    ).item(),\n",
        ")"
      ],
      "metadata": {
        "id": "HWxGj9yNrssb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specific Capability Test"
      ],
      "metadata": {
        "id": "GZD-MRjnr966"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompt = \"文章からキーワードを選び、その解釈を述べなさい。 新型コロナウイルスが広がらないようにするため、政府は建物の中で近くに人がいるときや話すときは、マスクをしたほうがいいと言っていました。3月13日からはこの考え方を変えて、マスクをするかどうか自分で決めるようにすると言いました。 しかし、病院やお年寄りの施設などに行くときや、混んでいる電車やバスに乗るときは、マスクをしたほうがいいと言っています。 学校の中では、4月1日からマスクをするように言わないことにします。その前でも卒業式ではマスクをしなくてもいいと言っています。 政府は、会社や学校などで困らないように、どんなときにマスクが必要かをしっかり伝えると言っています。\"\n",
        "example_answer = \"この文章に含まれるキーワードとその解釈は以下です。\"\n",
        "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
        "\n",
        "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
        "tokens = model.to_tokens(example_prompt)\n",
        "sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
        "    cache[sparse_autoencoder.cfg.hook_point]\n",
        ")"
      ],
      "metadata": {
        "id": "7ma7W3QZr9uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Feature Interfaces\n"
      ],
      "metadata": {
        "id": "xipsdj_ssCpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom Japanese dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"CohereForAI/aya_dataset\")\n",
        "\n",
        "dataset = dataset.filter(lambda example: example['language'] == 'Japanese')\n",
        "dataset = dataset.rename_column(\"inputs\", \"text\")\n",
        "\n",
        "dataset.push_to_hub(\"cohere-jp\")"
      ],
      "metadata": {
        "id": "PbBAcJ07CPHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vals, inds = torch.topk(feature_acts[0, -1].detach().cpu(), 10)\n",
        "px.bar(x=[str(i) for i in inds], y=vals).show()\n",
        "\n",
        "vocab_dict = model.tokenizer.vocab\n",
        "vocab_dict = {\n",
        "    v: k.replace(\"Ġ\", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()\n",
        "}\n",
        "\n",
        "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
        "if not vocab_dict_filepath.exists():\n",
        "    with open(vocab_dict_filepath, \"w\") as f:\n",
        "        json.dump(vocab_dict, f)\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "data = load_dataset(\n",
        "    \"kcoopermiller/cohere-jp\", split=\"train\"\n",
        ")  # currently use this dataset to avoid deal with tokenization while streaming\n",
        "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
        "tokenized_data = tokenized_data.shuffle(42)\n",
        "all_tokens = tokenized_data[\"tokens\"]\n",
        "\n",
        "\n",
        "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
        "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
        "\n",
        "total_batch_size = 4096 * 5\n",
        "feature_idx = list(inds.flatten().cpu().numpy())\n",
        "# max_batch_size = 512\n",
        "# total_batch_size = 16384\n",
        "# feature_idx = list(range(1000))\n",
        "\n",
        "\n",
        "feature_vis_params = SaeVisConfig(\n",
        "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
        "    minibatch_size_features=256,\n",
        "    minibatch_size_tokens=64,\n",
        "    features=feature_idx,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tokens = all_tokens[:total_batch_size]\n",
        "\n",
        "feature_data: Dict[int, FeatureData] = get_feature_data(\n",
        "    encoder=sparse_autoencoder,\n",
        "    model=model,\n",
        "    tokens=tokens,\n",
        "    cfg=feature_vis_params\n",
        ")\n",
        "\n",
        "feature_data.model = model\n",
        "\n",
        "for test_idx in list(inds.flatten().cpu().numpy()):\n",
        "    feature_data.save_feature_centric_vis(\n",
        "        f\"data_{test_idx:04}.html\",\n",
        "        feature_idx=test_idx\n",
        "    )"
      ],
      "metadata": {
        "id": "EsrsTCjgsCi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mor2hnqmAho5"
      },
      "source": [
        "# Merging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -q https://github.com/cg123/mergekit.git\n",
        "!python3 -m venv mergekit/env\n",
        "!source mergekit/env/bin/activate && pip install -q -e ./mergekit"
      ],
      "metadata": {
        "id": "3hOHTNv7yMpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPrH-gYGAvYU"
      },
      "source": [
        "## Frankenmerge / Passthrough\n",
        "*concatenate layers from both models*\n",
        "\n",
        "You can also use [MergeKit GUI Space](https://huggingface.co/spaces/arcee-ai/mergekit-gui) to merge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2rhfIUHAyVp"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"MATS-frankenmerge\"\n",
        "yaml_config = \"\"\"\n",
        "slices:\n",
        "  - sources:\n",
        "    - model: llm-jp/llm-jp-1.3b-v1.0\n",
        "      layer_range: [0, 24]\n",
        "  - sources:\n",
        "    - model: Qwen/Qwen1.5-0.5B\n",
        "      layer_range: [2]\n",
        "merge_method: passthrough\n",
        "dtype: bfloat16\n",
        "\"\"\"\n",
        "\n",
        "# Save config as yaml file\n",
        "with open('merge.yaml', 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(yaml_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mergekit && source env/bin/activate && mergekit-yaml ../merge.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle"
      ],
      "metadata": {
        "id": "XO-8Oj3-c-C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import ModelCard, ModelCardData, HfApi\n",
        "from jinja2 import Template\n",
        "from google.colab import userdata\n",
        "\n",
        "username = \"kcoopermiller\"\n",
        "\n",
        "template_text = \"\"\"\n",
        "---\n",
        "license: apache-2.0\n",
        "tags:\n",
        "- merge\n",
        "- mergekit\n",
        "- lazymergekit\n",
        "{%- for model in models %}\n",
        "- {{ model }}\n",
        "{%- endfor %}\n",
        "---\n",
        "\n",
        "# {{ model_name }}\n",
        "\n",
        "{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
        "\n",
        "{%- for model in models %}\n",
        "* [{{ model }}](https://huggingface.co/{{ model }})\n",
        "{%- endfor %}\n",
        "\n",
        "## 🧩 Configuration\n",
        "\n",
        "\\```yaml\n",
        "{{- yaml_config -}}\n",
        "\\```\n",
        "\"\"\"\n",
        "\n",
        "# Create a Jinja template object\n",
        "jinja_template = Template(template_text.strip())\n",
        "\n",
        "# Get list of models from config\n",
        "data = yaml.safe_load(yaml_config)\n",
        "if \"models\" in data:\n",
        "    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\n",
        "elif \"parameters\" in data:\n",
        "    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\n",
        "elif \"slices\" in data:\n",
        "    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\n",
        "else:\n",
        "    raise Exception(\"No models or slices found in yaml config\")\n",
        "\n",
        "# Fill the template\n",
        "content = jinja_template.render(\n",
        "    model_name=MODEL_NAME,\n",
        "    models=models,\n",
        "    yaml_config=yaml_config,\n",
        "    username=username,\n",
        ")\n",
        "\n",
        "# Save the model card\n",
        "card = ModelCard(content)\n",
        "card.save('merge/README.md')\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "api.upload_folder(\n",
        "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path=\"merge\",\n",
        ")"
      ],
      "metadata": {
        "id": "oM8ozDhKdW3M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "JOQj20Sc6nVY",
        "mor2hnqmAho5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}