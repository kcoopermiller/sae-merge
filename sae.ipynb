{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am_6oGvv6jkq"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YydISNwuaCvu"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli login\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"CPU\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKDgKeGUT1Y"
      },
      "source": [
        "# Sparse Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I6tHUGfyucj"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n",
        "\n",
        "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
        "from sae_lens.training.lm_runner import language_model_sae_runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRyD9P8vhMaa"
      },
      "outputs": [],
      "source": [
        "# https://jbloomaus.github.io/SAELens/training_saes/\n",
        "cfg = LanguageModelSAERunnerConfig(\n",
        "\n",
        "    # Data Generating Function (Model + Training Distibuion)\n",
        "    model_name = \"qwen1.5-0.5b\",\n",
        "    hook_point = \"blocks.18.hook_resid_pre\",\n",
        "    hook_point_layer = 18,\n",
        "    d_in = 1024, # https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html\n",
        "    dataset_path = \"Skylion007/openwebtext\",\n",
        "    is_dataset_tokenized=False,\n",
        "\n",
        "    # SAE Parameters\n",
        "    expansion_factor = 64,\n",
        "    b_dec_init_method = \"geometric_median\",\n",
        "\n",
        "    # Training Parameters\n",
        "    lr = 0.0004,\n",
        "    l1_coefficient = 0.00008,\n",
        "    lr_scheduler_name=\"constant\", # constantwithwarmup not supported?\n",
        "    train_batch_size = 4096,\n",
        "    context_size = 128,\n",
        "    feature_sampling_window = 1000,\n",
        "    dead_feature_window=5000,\n",
        "    dead_feature_threshold = 1e-6,\n",
        "\n",
        "    # WANDB\n",
        "    log_to_wandb = True,\n",
        "    wandb_project= \"mats_sae_training_qwen\",\n",
        "    wandb_entity = None,\n",
        "    wandb_log_frequency=100,\n",
        "\n",
        "    # Misc\n",
        "    device = \"cuda\",\n",
        "    seed = 42,\n",
        "    n_checkpoints = 10,\n",
        "    checkpoint_path = \"checkpoints\",\n",
        "    dtype = torch.float32,\n",
        "    )\n",
        "\n",
        "sparse_autoencoder = language_model_sae_runner(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-U2hVwq9L_"
      },
      "source": [
        "## Upload to HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIm4pugaownw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "uuid_str = \"pqs59n3e\"\n",
        "repo_id = \"kcoopermiller/qwen1.5-0.5b-saes\"\n",
        "local_folder = f\"checkpoints/{uuid_str}\"\n",
        "hf_folder = f\"{uuid_str}\"\n",
        "api.upload_folder(\n",
        "    folder_path=local_folder,\n",
        "    path_in_repo=hf_folder,\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrC3Evkfq3wg"
      },
      "source": [
        "## Evaluating the SAEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy3EcxQFgC4Y"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import plotly.express as px\n",
        "from transformer_lens import utils\n",
        "from datasets import load_dataset\n",
        "from typing import Dict\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download\n",
        "from functools import partial\n",
        "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
        "from sae_vis.data_fetching_fns import get_feature_data, FeatureData\n",
        "from sae_vis.data_config_classes import SaeVisConfig\n",
        "torch.set_grad_enabled(False)\n",
        "sys.path.append(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk7pZpt-iD93"
      },
      "outputs": [],
      "source": [
        "REPO_ID = \"kcoopermiller/qwen1.5-0.5b-saes\"\n",
        "\n",
        "layer = 2 # 2 or 18\n",
        "checkpoint = \"crujwafo\" if layer == 2 else \"pqs59n3e\"\n",
        "FILENAME = f\"{checkpoint}/final_sae_group_qwen1.5-0.5b_blocks.{layer}.hook_resid_pre_65536.pt\"\n",
        "\n",
        "path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "\n",
        "model, sparse_autoencoders, activation_store = (\n",
        "    LMSparseAutoencoderSessionloader.load_session_from_pretrained(path=path)\n",
        ")\n",
        "sparse_autoencoders.eval()\n",
        "sparse_autoencoder = list(sparse_autoencoders)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_wTDMvVrtIJ"
      },
      "source": [
        "### L0 Test and Reconstruction Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWxGj9yNrssb"
      },
      "outputs": [],
      "source": [
        "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
        "with torch.no_grad():\n",
        "    batch_tokens = activation_store.get_batch_tokens()\n",
        "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
        "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
        "        cache[sparse_autoencoder.cfg.hook_point]\n",
        "    )\n",
        "    del cache\n",
        "\n",
        "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
        "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
        "    print(\"average l0\", l0.mean().item())\n",
        "    px.histogram(l0.flatten().cpu().numpy()).show()\n",
        "\n",
        "# next we want to do a reconstruction test.\n",
        "def reconstr_hook(activation, hook, sae_out):\n",
        "    return sae_out\n",
        "\n",
        "\n",
        "def zero_abl_hook(activation, hook):\n",
        "    return torch.zeros_like(activation)\n",
        "\n",
        "\n",
        "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
        "print(\n",
        "    \"reconstr\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        fwd_hooks=[\n",
        "            (\n",
        "                utils.get_act_name(\"resid_pre\", 10),\n",
        "                partial(reconstr_hook, sae_out=sae_out),\n",
        "            )\n",
        "        ],\n",
        "        return_type=\"loss\",\n",
        "    ).item(),\n",
        ")\n",
        "print(\n",
        "    \"Zero\",\n",
        "    model.run_with_hooks(\n",
        "        batch_tokens,\n",
        "        return_type=\"loss\",\n",
        "        fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
        "    ).item(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZD-MRjnr966"
      },
      "source": [
        "### Specific Capability Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ma7W3QZr9uV"
      },
      "outputs": [],
      "source": [
        "example_prompt = \"What is a common dog name?\"\n",
        "example_answer = \"A common dog name is Max.\"\n",
        "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
        "\n",
        "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
        "tokens = model.to_tokens(example_prompt)\n",
        "sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
        "    cache[sparse_autoencoder.cfg.hook_point]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xipsdj_ssCpy"
      },
      "source": [
        "### Generating Feature Interfaces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsrsTCjgsCi8"
      },
      "outputs": [],
      "source": [
        "vals, inds = torch.topk(feature_acts[0, -1].detach().cpu(), 10)\n",
        "px.bar(x=[str(i) for i in inds], y=vals).show()\n",
        "\n",
        "vocab_dict = model.tokenizer.vocab\n",
        "vocab_dict = {\n",
        "    v: k.replace(\"Ä \", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()\n",
        "}\n",
        "\n",
        "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
        "if not vocab_dict_filepath.exists():\n",
        "    with open(vocab_dict_filepath, \"w\") as f:\n",
        "        json.dump(vocab_dict, f)\n",
        "\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "data = load_dataset(\n",
        "    \"NeelNanda/c4-code-20k\", split=\"train\"\n",
        ")  # currently use this dataset to avoid deal with tokenization while streaming\n",
        "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
        "tokenized_data = tokenized_data.shuffle(42)\n",
        "all_tokens = tokenized_data[\"tokens\"]\n",
        "\n",
        "\n",
        "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
        "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
        "\n",
        "total_batch_size = 4096 * 5\n",
        "feature_idx = list(inds.flatten().cpu().numpy())\n",
        "# max_batch_size = 512\n",
        "# total_batch_size = 16384\n",
        "# feature_idx = list(range(1000))\n",
        "\n",
        "\n",
        "feature_vis_params = SaeVisConfig(\n",
        "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
        "    minibatch_size_features=256,\n",
        "    minibatch_size_tokens=64,\n",
        "    features=feature_idx,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "tokens = all_tokens[:total_batch_size]\n",
        "\n",
        "feature_data: Dict[int, FeatureData] = get_feature_data(\n",
        "    encoder=sparse_autoencoder,\n",
        "    model=model,\n",
        "    tokens=tokens,\n",
        "    cfg=feature_vis_params\n",
        ")\n",
        "\n",
        "feature_data.model = model\n",
        "\n",
        "for test_idx in list(inds.flatten().cpu().numpy()):\n",
        "    feature_data.save_feature_centric_vis(\n",
        "        f\"data_{test_idx:04}.html\",\n",
        "        feature_idx=test_idx\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JOQj20Sc6nVY",
        "mor2hnqmAho5"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
